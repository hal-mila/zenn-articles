---
title: "【論文漫画解説】自己蒸留が継続学習を可能にする：破滅的忘却を防ぐための新たなアプローチ"
emoji: "📘"
type: "idea"
topics: ["arxiv", "機械学習", "論文解説", "AI", "gemini"]
published: true
---

この記事では、機械学習分野の最新の arXiv 論文をもとに、
概要を日本語で分かりやすく解説し、その内容を4コマ漫画形式の画像として生成しています。
本記事の文書や漫画の内容はあくまでAIを活用した要約であり、間違いを含む可能性があることはご了承ください。


> **論文情報**  
> - arXiv ID: `2601.19897v1`  
> - 著者: Idan Shenfeld, Mehul Damani, Jonas Hübotter, Pulkit Agrawal  
> - arXiv: https://arxiv.org/abs/2601.19897v1  
> - PDF: https://arxiv.org/pdf/2601.19897v1.pdf

---

## 📘 漫画でざっくり理解！

![論文漫画](/images/arxiv-2601-19897v1.png)

---

## 🧠 論文の内容をやさしく解説

大規模言語モデルなどの基盤モデルにおいて、新しい知識を学習させると以前の知識を忘れてしまう「破滅的忘却」は、開発者にとって大きな課題です。これを防ぐ「継続学習」には、通常、複雑な報酬設定が必要な強化学習か、単に正解データを覚えさせる教師あり微調整（SFT）が使われますが、前者は実装コストが高く、後者は忘却が起きやすいという欠点がありました。

この論文では、**SDFT (Self-Distillation Fine-Tuning)** という新しい手法を提案しています。これは、モデルに「手本（デモンストレーション）」を見せて推論させ、その「自分自身が生成した出力」を教師データとして再学習する手法です。外部から与えられた正解をただ丸暗記するのではなく、自分の現在の能力を使って導き出した答えを学ぶ（オンポリシー学習）ことで、モデル内部の既存知識を大きく破壊することなく、新しいスキルを定着させます。

この手法の革新性は、強化学習のような特別な報酬関数を設計することなく、手本データさえあれば「忘れにくい学習」を実現した点にあります。実験では、従来のSFTと比較して新しいタスクの精度が高く、かつ過去のタスクも忘れないことが実証されました。これにより、一つのモデルに対して順次新しい機能を追加していく運用が、より低コストかつ実用的になります。

**研究のポイント**
*   **課題:** モデルに追加学習を行うと、過去のスキルを忘れてしまう（破滅的忘却）。
*   **解決策:** 手本を見てモデル自身が生成した回答を教材にする「自己蒸留」アプローチを採用。
*   **メリット:** 従来の微調整（SFT）よりも新タスクの習得精度が高く、かつ過去の知識も維持できるため、継続的な機能追加が容易になる。

---

