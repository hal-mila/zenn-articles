---
title: "【論文漫画解説】FLOPsの再利用：非常にオフポリシーなプレフィックスを条件付けた難問における強化学習のスケーリング"
emoji: "📘"
type: "idea"
topics: ["arxiv", "機械学習", "論文解説", "AI", "gemini"]
published: true
---

この記事では、機械学習分野の最新の arXiv 論文をもとに、
概要を日本語で分かりやすく解説し、その内容を4コマ漫画形式の画像として生成しています。
本記事の文書や漫画の内容はあくまでAIを活用した要約であり、間違いを含む可能性があることはご了承ください。


> **論文情報**  
> - arXiv ID: `2601.18795v1`  
> - 著者: Amrith Setlur, Zijian Wang, Andrew Cohen, Paria Rashidinejad, Sang Michael Xie  
> - arXiv: https://arxiv.org/abs/2601.18795v1  
> - PDF: https://arxiv.org/pdf/2601.18795v1.pdf

---

## 📘 漫画でざっくり理解！

![論文漫画](/images/arxiv-2601-18795v1.png)

---

## 🧠 論文の内容をやさしく解説

大規模言語モデル（LLM）に数学や論理パズルなどの複雑な推論問題を強化学習させる際、モデルが自力で正解に辿り着く確率は非常に低くなります。正解できないと報酬（フィードバック）が得られないため、学習が停滞し、膨大な計算リソース（FLOPs）が無駄になるという課題がありました。

本論文では、過去の推論や他のモデルが生成した「成功した推論プロセス（オフポリシーデータ）」を有効活用する「PrefixRL」という手法を提案しています。
具体的には、成功データの「冒頭部分（プレフィックス）」だけをヒントとしてモデルに与え、残りの続きを現在のモデルに生成させて学習させます。これにより、完全に自力で解くよりも難易度が下がり、適切な学習フィードバックを得やすくなります。

従来の手法では過去のデータをそのまま学習に使うと挙動が不安定になりがちでしたが、本手法は「続きを書かせる」ことでその問題を回避しました。特筆すべきは「逆汎化（Back-generalization）」という現象の発見です。ヒント（プレフィックス）付きの問題で訓練したにもかかわらず、ヒントなしの未知の問題を解く能力も向上しました。結果として、既存の強力な手法と比較して学習速度が2倍、最終的な性能が3倍に向上しています。

**研究のポイント**
*   **計算資源の再利用:** 過去の成功データを「書き出しのヒント」として再利用し、学習効率を劇的に改善。
*   **学習の安定化:** 過去データを教師データとして強制するのではなく、続きを生成させることで強化学習の不安定さを解消。
*   **逆汎化の発見:** 「途中から解く」練習をすることで、結果的に「最初から自力で解く」能力も向上することを確認。

---

